<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>VM_BIND locking &#8212; The Linux Kernel  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=a152c8ac" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="TODO list" href="todo.html" />
    <link rel="prev" title="Asynchronous VM_BIND" href="drm-vm-bind-async.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/logo.svg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">The Linux Kernel</a></h1>



<p class="blurb">6.14.0</p>







<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script><!-- SPDX-License-Identifier: GPL-2.0 -->

<p>
<h3 class="kernel-toc-contents">Contents</h3>
<input type="checkbox" class="kernel-toc-toggle" id = "kernel-toc-toggle" checked>
<label class="kernel-toc-title" for="kernel-toc-toggle"></label>

<div class="kerneltoc" id="kerneltoc">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../process/development-process.html">Development process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../process/submitting-patches.html">Submitting patches</a></li>
<li class="toctree-l1"><a class="reference internal" href="../process/code-of-conduct.html">Code of conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../maintainer/index.html">Maintainer handbook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../process/index.html">All development-process docs</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../core-api/index.html">Core API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../driver-api/index.html">Driver APIs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../subsystem-apis.html">Subsystems</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#core-subsystems">Core subsystems</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../subsystem-apis.html#human-interfaces">Human interfaces</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../input/index.html">Input Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hid/index.html">Human Interface Devices (HID)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sound/index.html">Sound Subsystem Documentation</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">GPU Driver Developer’s Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../fb/index.html">Frame Buffer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../leds/index.html">LEDs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#networking-interfaces">Networking interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#storage-interfaces">Storage interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#other-subsystems">Other subsystems</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../locking/index.html">Locking</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../process/license-rules.html">Licensing rules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../doc-guide/index.html">Writing documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev-tools/index.html">Development tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev-tools/testing-overview.html">Testing guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel-hacking/index.html">Hacking guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trace/index.html">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fault-injection/index.html">Fault injection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../livepatch/index.html">Livepatching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rust/index.html">Rust</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../admin-guide/index.html">Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kbuild/index.html">Build system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin-guide/reporting-issues.html">Reporting issues</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/index.html">Userspace tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../userspace-api/index.html">Userspace API</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../firmware-guide/index.html">Firmware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../devicetree/index.html">Firmware and Devicetree</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../arch/index.html">CPU architectures</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../staging/index.html">Unsorted documentation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../translations/index.html">Translations</a></li>
</ul>

</div>

<script type="text/javascript"> <!--
  var sbar = document.getElementsByClassName("sphinxsidebar")[0];
  let currents = document.getElementsByClassName("current")
  if (currents.length) {
    sbar.scrollTop = currents[currents.length - 1].offsetTop;
  }
  --> </script>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/gpu/drm-vm-bind-locking.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <!-- SPDX-License-Identifier: GPL-2.0 -->
<!-- Copyright © 2023, Oracle and/or its affiliates. -->


<section id="vm-bind-locking">
<h1>VM_BIND locking<a class="headerlink" href="#vm-bind-locking" title="Link to this heading">¶</a></h1>
<p>This document attempts to describe what’s needed to get VM_BIND locking right,
including the userptr mmu_notifier locking. It also discusses some
optimizations to get rid of the looping through of all userptr mappings and
external / shared object mappings that is needed in the simplest
implementation. In addition, there is a section describing the VM_BIND locking
required for implementing recoverable pagefaults.</p>
<section id="the-drm-gpuvm-set-of-helpers">
<h2>The DRM GPUVM set of helpers<a class="headerlink" href="#the-drm-gpuvm-set-of-helpers" title="Link to this heading">¶</a></h2>
<p>There is a set of helpers for drivers implementing VM_BIND, and this
set of helpers implements much, but not all of the locking described
in this document. In particular, it is currently lacking a userptr
implementation. This document does not intend to describe the DRM GPUVM
implementation in detail, but it is covered in <a class="reference internal" href="drm-mm.html#drm-gpuvm"><span class="std std-ref">its own
documentation</span></a>. It is highly recommended for any driver
implementing VM_BIND to use the DRM GPUVM helpers and to extend it if
common functionality is missing.</p>
</section>
<section id="nomenclature">
<h2>Nomenclature<a class="headerlink" href="#nomenclature" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_vm</span></code>: Abstraction of a virtual GPU address space with
meta-data. Typically one per client (DRM file-private), or one per
execution context.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_vma</span></code>: Abstraction of a GPU address range within a gpu_vm with
associated meta-data. The backing storage of a gpu_vma can either be
a GEM object or anonymous or page-cache pages mapped also into the CPU
address space for the process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_vm_bo</span></code>: Abstracts the association of a GEM object and
a VM. The GEM object maintains a list of gpu_vm_bos, where each gpu_vm_bo
maintains a list of gpu_vmas.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">userptr</span> <span class="pre">gpu_vma</span> <span class="pre">or</span> <span class="pre">just</span> <span class="pre">userptr</span></code>: A gpu_vma, whose backing store
is anonymous or page-cache pages as described above.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">revalidating</span></code>: Revalidating a gpu_vma means making the latest version
of the backing store resident and making sure the gpu_vma’s
page-table entries point to that backing store.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dma_fence</span></code>: A <a class="reference internal" href="../driver-api/dma-buf.html#c.dma_fence" title="dma_fence"><code class="xref c c-struct docutils literal notranslate"><span class="pre">struct</span> <span class="pre">dma_fence</span></code></a> that is similar to a struct completion
and which tracks GPU activity. When the GPU activity is finished,
the dma_fence signals. Please refer to the <code class="docutils literal notranslate"><span class="pre">DMA</span> <span class="pre">Fences</span></code> section of
the <a class="reference internal" href="../driver-api/dma-buf.html"><span class="doc">dma-buf doc</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dma_resv</span></code>: A <a class="reference internal" href="../driver-api/dma-buf.html#c.dma_resv" title="dma_resv"><code class="xref c c-struct docutils literal notranslate"><span class="pre">struct</span> <span class="pre">dma_resv</span></code></a> (a.k.a reservation object) that is used
to track GPU activity in the form of multiple dma_fences on a
gpu_vm or a GEM object. The dma_resv contains an array / list
of dma_fences and a lock that needs to be held when adding
additional dma_fences to the dma_resv. The lock is of a type that
allows deadlock-safe locking of multiple dma_resvs in arbitrary
order. Please refer to the <code class="docutils literal notranslate"><span class="pre">Reservation</span> <span class="pre">Objects</span></code> section of the
<a class="reference internal" href="../driver-api/dma-buf.html"><span class="doc">dma-buf doc</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exec</span> <span class="pre">function</span></code>: An exec function is a function that revalidates all
affected gpu_vmas, submits a GPU command batch and registers the
dma_fence representing the GPU command’s activity with all affected
dma_resvs. For completeness, although not covered by this document,
it’s worth mentioning that an exec function may also be the
revalidation worker that is used by some drivers in compute /
long-running mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">local</span> <span class="pre">object</span></code>: A GEM object which is only mapped within a
single VM. Local GEM objects share the gpu_vm’s dma_resv.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">external</span> <span class="pre">object</span></code>: a.k.a shared object: A GEM object which may be shared
by multiple gpu_vms and whose backing storage may be shared with
other drivers.</p></li>
</ul>
</section>
<section id="locks-and-locking-order">
<h2>Locks and locking order<a class="headerlink" href="#locks-and-locking-order" title="Link to this heading">¶</a></h2>
<p>One of the benefits of VM_BIND is that local GEM objects share the gpu_vm’s
dma_resv object and hence the dma_resv lock. So, even with a huge
number of local GEM objects, only one lock is needed to make the exec
sequence atomic.</p>
<p>The following locks and locking orders are used:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;lock</span></code> (optionally an rwsem). Protects the gpu_vm’s
data structure keeping track of gpu_vmas. It can also protect the
gpu_vm’s list of userptr gpu_vmas. With a CPU mm analogy this would
correspond to the mmap_lock. An rwsem allows several readers to walk
the VM tree concurrently, but the benefit of that concurrency most
likely varies from driver to driver.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">userptr_seqlock</span></code>. This lock is taken in read mode for each
userptr gpu_vma on the gpu_vm’s userptr list, and in write mode during mmu
notifier invalidation. This is not a real seqlock but described in
<code class="docutils literal notranslate"><span class="pre">mm/mmu_notifier.c</span></code> as a “Collision-retry read-side/write-side
‘lock’ a lot like a seqcount. However this allows multiple
write-sides to hold it at once...”. The read side critical section
is enclosed by <code class="docutils literal notranslate"><span class="pre">mmu_interval_read_begin()</span> <span class="pre">/</span>
<span class="pre">mmu_interval_read_retry()</span></code> with <code class="docutils literal notranslate"><span class="pre">mmu_interval_read_begin()</span></code>
sleeping if the write side is held.
The write side is held by the core mm while calling mmu interval
invalidation notifiers.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;resv</span></code> lock. Protects the gpu_vm’s list of gpu_vmas needing
rebinding, as well as the residency state of all the gpu_vm’s local
GEM objects.
Furthermore, it typically protects the gpu_vm’s list of evicted and
external GEM objects.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;userptr_notifier_lock</span></code>. This is an rwsem that is
taken in read mode during exec and write mode during a mmu notifier
invalidation. The userptr notifier lock is per gpu_vm.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gem_object-&gt;gpuva_lock</span></code> This lock protects the GEM object’s
list of gpu_vm_bos. This is usually the same lock as the GEM
object’s dma_resv, but some drivers protects this list differently,
see below.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu_vm</span> <span class="pre">list</span> <span class="pre">spinlocks</span></code>. With some implementations they are needed
to be able to update the gpu_vm evicted- and external object
list. For those implementations, the spinlocks are grabbed when the
lists are manipulated. However, to avoid locking order violations
with the dma_resv locks, a special scheme is needed when iterating
over the lists.</p></li>
</ul>
</section>
<section id="protection-and-lifetime-of-gpu-vm-bos-and-gpu-vmas">
<span id="gpu-vma-lifetime"></span><h2>Protection and lifetime of gpu_vm_bos and gpu_vmas<a class="headerlink" href="#protection-and-lifetime-of-gpu-vm-bos-and-gpu-vmas" title="Link to this heading">¶</a></h2>
<p>The GEM object’s list of gpu_vm_bos, and the gpu_vm_bo’s list of gpu_vmas
is protected by the <code class="docutils literal notranslate"><span class="pre">gem_object-&gt;gpuva_lock</span></code>, which is typically the
same as the GEM object’s dma_resv, but if the driver
needs to access these lists from within a dma_fence signalling
critical section, it can instead choose to protect it with a
separate lock, which can be locked from within the dma_fence signalling
critical section. Such drivers then need to pay additional attention
to what locks need to be taken from within the loop when iterating
over the gpu_vm_bo and gpu_vma lists to avoid locking-order violations.</p>
<p>The DRM GPUVM set of helpers provide lockdep asserts that this lock is
held in relevant situations and also provides a means of making itself
aware of which lock is actually used: <a class="reference internal" href="drm-mm.html#c.drm_gem_gpuva_set_lock" title="drm_gem_gpuva_set_lock"><code class="xref c c-func docutils literal notranslate"><span class="pre">drm_gem_gpuva_set_lock()</span></code></a>.</p>
<p>Each gpu_vm_bo holds a reference counted pointer to the underlying GEM
object, and each gpu_vma holds a reference counted pointer to the
gpu_vm_bo. When iterating over the GEM object’s list of gpu_vm_bos and
over the gpu_vm_bo’s list of gpu_vmas, the <code class="docutils literal notranslate"><span class="pre">gem_object-&gt;gpuva_lock</span></code> must
not be dropped, otherwise, gpu_vmas attached to a gpu_vm_bo may
disappear without notice since those are not reference-counted. A
driver may implement its own scheme to allow this at the expense of
additional complexity, but this is outside the scope of this document.</p>
<p>In the DRM GPUVM implementation, each gpu_vm_bo and each gpu_vma
holds a reference count on the gpu_vm itself. Due to this, and to avoid circular
reference counting, cleanup of the gpu_vm’s gpu_vmas must not be done from the
gpu_vm’s destructor. Drivers typically implements a gpu_vm close
function for this cleanup. The gpu_vm close function will abort gpu
execution using this VM, unmap all gpu_vmas and release page-table memory.</p>
</section>
<section id="revalidation-and-eviction-of-local-objects">
<h2>Revalidation and eviction of local objects<a class="headerlink" href="#revalidation-and-eviction-of-local-objects" title="Link to this heading">¶</a></h2>
<p>Note that in all the code examples given below we use simplified
pseudo-code. In particular, the dma_resv deadlock avoidance algorithm
as well as reserving memory for dma_resv fences is left out.</p>
<section id="revalidation">
<h3>Revalidation<a class="headerlink" href="#revalidation" title="Link to this heading">¶</a></h3>
<p>With VM_BIND, all local objects need to be resident when the gpu is
executing using the gpu_vm, and the objects need to have valid
gpu_vmas set up pointing to them. Typically, each gpu command buffer
submission is therefore preceded with a re-validation section:</p>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="n">dma_resv_lock</span><span class="p">(</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>

<span class="c1">// Validation section starts here.</span>
<span class="n">for_each_gpu_vm_bo_on_evict_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">evict_list</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">validate_gem_bo</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="o">-&gt;</span><span class="n">gem_bo</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// The following list iteration needs the Gem object&#39;s</span>
<span class="w">        </span><span class="c1">// dma_resv to be held (it protects the gpu_vm_bo&#39;s list of</span>
<span class="w">        </span><span class="c1">// gpu_vmas, but since local gem objects share the gpu_vm&#39;s</span>
<span class="w">        </span><span class="c1">// dma_resv, it is already held at this point.</span>
<span class="w">        </span><span class="n">for_each_gpu_vma_of_gpu_vm_bo</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">)</span>
<span class="w">               </span><span class="n">move_gpu_vma_to_rebind_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">rebind_list</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">for_each_gpu_vma_on_rebind_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu</span><span class="w"> </span><span class="n">vm</span><span class="o">-&gt;</span><span class="n">rebind_list</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">rebind_gpu_vma</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">);</span>
<span class="w">        </span><span class="n">remove_gpu_vma_from_rebind_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">);</span>
<span class="p">}</span>
<span class="c1">// Validation section ends here, and job submission starts.</span>

<span class="n">add_dependencies</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_job</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">job_dma_fence</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gpu_submit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_job</span><span class="p">));</span>

<span class="n">add_dma_fence</span><span class="p">(</span><span class="n">job_dma_fence</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">dma_resv_unlock</span><span class="p">(</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
</pre></div>
</div>
<p>The reason for having a separate gpu_vm rebind list is that there
might be userptr gpu_vmas that are not mapping a buffer object that
also need rebinding.</p>
</section>
<section id="eviction">
<h3>Eviction<a class="headerlink" href="#eviction" title="Link to this heading">¶</a></h3>
<p>Eviction of one of these local objects will then look similar to the
following:</p>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_object_from_lru</span><span class="p">();</span>

<span class="n">dma_resv_lock</span><span class="p">(</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">for_each_gpu_vm_bo_of_obj</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">);</span>
<span class="w">        </span><span class="n">add_gpu_vm_bo_to_evict_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">evict_list</span><span class="p">);</span>

<span class="n">add_dependencies</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eviction_job</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">job_dma_fence</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gpu_submit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eviction_job</span><span class="p">);</span>
<span class="n">add_dma_fence</span><span class="p">(</span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">,</span><span class="w"> </span><span class="n">job_dma_fence</span><span class="p">);</span>

<span class="n">dma_resv_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">put_object</span><span class="p">(</span><span class="n">obj</span><span class="p">);</span>
</pre></div>
</div>
<p>Note that since the object is local to the gpu_vm, it will share the gpu_vm’s
dma_resv lock such that <code class="docutils literal notranslate"><span class="pre">obj-&gt;resv</span> <span class="pre">==</span> <span class="pre">gpu_vm-&gt;resv</span></code>.
The gpu_vm_bos marked for eviction are put on the gpu_vm’s evict list,
which is protected by <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;resv</span></code>. During eviction all local
objects have their dma_resv locked and, due to the above equality, also
the gpu_vm’s dma_resv protecting the gpu_vm’s evict list is locked.</p>
<p>With VM_BIND, gpu_vmas don’t need to be unbound before eviction,
since the driver must ensure that the eviction blit or copy will wait
for GPU idle or depend on all previous GPU activity. Furthermore, any
subsequent attempt by the GPU to access freed memory through the
gpu_vma will be preceded by a new exec function, with a revalidation
section which will make sure all gpu_vmas are rebound. The eviction
code holding the object’s dma_resv while revalidating will ensure a
new exec function may not race with the eviction.</p>
<p>A driver can be implemented in such a way that, on each exec function,
only a subset of vmas are selected for rebind.  In this case, all vmas that are
<em>not</em> selected for rebind must be unbound before the exec
function workload is submitted.</p>
</section>
</section>
<section id="locking-with-external-buffer-objects">
<h2>Locking with external buffer objects<a class="headerlink" href="#locking-with-external-buffer-objects" title="Link to this heading">¶</a></h2>
<p>Since external buffer objects may be shared by multiple gpu_vm’s they
can’t share their reservation object with a single gpu_vm. Instead
they need to have a reservation object of their own. The external
objects bound to a gpu_vm using one or many gpu_vmas are therefore put on a
per-gpu_vm list which is protected by the gpu_vm’s dma_resv lock or
one of the <a class="reference internal" href="#spinlock-iteration"><span class="std std-ref">gpu_vm list spinlocks</span></a>. Once
the gpu_vm’s reservation object is locked, it is safe to traverse the
external object list and lock the dma_resvs of all external
objects. However, if instead a list spinlock is used, a more elaborate
iteration scheme needs to be used.</p>
<p>At eviction time, the gpu_vm_bos of <em>all</em> the gpu_vms an external
object is bound to need to be put on their gpu_vm’s evict list.
However, when evicting an external object, the dma_resvs of the
gpu_vms the object is bound to are typically not held. Only
the object’s private dma_resv can be guaranteed to be held. If there
is a ww_acquire context at hand at eviction time we could grab those
dma_resvs but that could cause expensive ww_mutex rollbacks. A simple
option is to just mark the gpu_vm_bos of the evicted gem object with
an <code class="docutils literal notranslate"><span class="pre">evicted</span></code> bool that is inspected before the next time the
corresponding gpu_vm evicted list needs to be traversed. For example, when
traversing the list of external objects and locking them. At that time,
both the gpu_vm’s dma_resv and the object’s dma_resv is held, and the
gpu_vm_bo marked evicted, can then be added to the gpu_vm’s list of
evicted gpu_vm_bos. The <code class="docutils literal notranslate"><span class="pre">evicted</span></code> bool is formally protected by the
object’s dma_resv.</p>
<p>The exec function becomes</p>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="n">dma_resv_lock</span><span class="p">(</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>

<span class="c1">// External object list is protected by the gpu_vm-&gt;resv lock.</span>
<span class="n">for_each_gpu_vm_bo_on_extobj_list</span><span class="p">(</span><span class="n">gpu_vm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">dma_resv_lock</span><span class="p">(</span><span class="n">gpu_vm_bo</span><span class="p">.</span><span class="n">gem_obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gpu_vm_bo_marked_evicted</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">))</span>
<span class="w">                </span><span class="n">add_gpu_vm_bo_to_evict_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">evict_list</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">for_each_gpu_vm_bo_on_evict_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">evict_list</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">validate_gem_bo</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="o">-&gt;</span><span class="n">gem_bo</span><span class="p">);</span>

<span class="w">        </span><span class="n">for_each_gpu_vma_of_gpu_vm_bo</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">)</span>
<span class="w">               </span><span class="n">move_gpu_vma_to_rebind_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">rebind_list</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">for_each_gpu_vma_on_rebind_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu</span><span class="w"> </span><span class="n">vm</span><span class="o">-&gt;</span><span class="n">rebind_list</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">rebind_gpu_vma</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">);</span>
<span class="w">        </span><span class="n">remove_gpu_vma_from_rebind_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">add_dependencies</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_job</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">job_dma_fence</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gpu_submit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_job</span><span class="p">));</span>

<span class="n">add_dma_fence</span><span class="p">(</span><span class="n">job_dma_fence</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">for_each_external_obj</span><span class="p">(</span><span class="n">gpu_vm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">obj</span><span class="p">)</span>
<span class="w">       </span><span class="n">add_dma_fence</span><span class="p">(</span><span class="n">job_dma_fence</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">dma_resv_unlock_all_resv_locks</span><span class="p">();</span>
</pre></div>
</div>
<p>And the corresponding shared-object aware eviction would look like:</p>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_object_from_lru</span><span class="p">();</span>

<span class="n">dma_resv_lock</span><span class="p">(</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">for_each_gpu_vm_bo_of_obj</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">object_is_vm_local</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
<span class="w">             </span><span class="n">add_gpu_vm_bo_to_evict_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">evict_list</span><span class="p">);</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">             </span><span class="n">mark_gpu_vm_bo_evicted</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm_bo</span><span class="p">);</span>

<span class="n">add_dependencies</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eviction_job</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">job_dma_fence</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gpu_submit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eviction_job</span><span class="p">);</span>
<span class="n">add_dma_fence</span><span class="p">(</span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">,</span><span class="w"> </span><span class="n">job_dma_fence</span><span class="p">);</span>

<span class="n">dma_resv_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">put_object</span><span class="p">(</span><span class="n">obj</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="accessing-the-gpu-vm-s-lists-without-the-dma-resv-lock-held">
<span id="spinlock-iteration"></span><h2>Accessing the gpu_vm’s lists without the dma_resv lock held<a class="headerlink" href="#accessing-the-gpu-vm-s-lists-without-the-dma-resv-lock-held" title="Link to this heading">¶</a></h2>
<p>Some drivers will hold the gpu_vm’s dma_resv lock when accessing the
gpu_vm’s evict list and external objects lists. However, there are
drivers that need to access these lists without the dma_resv lock
held, for example due to asynchronous state updates from within the
dma_fence signalling critical path. In such cases, a spinlock can be
used to protect manipulation of the lists. However, since higher level
sleeping locks need to be taken for each list item while iterating
over the lists, the items already iterated over need to be
temporarily moved to a private list and the spinlock released
while processing each item:</p>
<p>Due to the additional locking and atomic operations, drivers that <em>can</em>
avoid accessing the gpu_vm’s list outside of the dma_resv lock
might want to avoid also this iteration scheme. Particularly, if the
driver anticipates a large number of list items. For lists where the
anticipated number of list items is small, where list iteration doesn’t
happen very often or if there is a significant additional cost
associated with each iteration, the atomic operation overhead
associated with this type of iteration is, most likely, negligible. Note that
if this scheme is used, it is necessary to make sure this list
iteration is protected by an outer level lock or semaphore, since list
items are temporarily pulled off the list while iterating, and it is
also worth mentioning that the local list <code class="docutils literal notranslate"><span class="pre">still_in_list</span></code> should
also be considered protected by the <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;list_lock</span></code>, and it is
thus possible that items can be removed also from the local list
concurrently with list iteration.</p>
<p>Please refer to the <a class="reference internal" href="drm-mm.html#drm-gpuvm-locking"><span class="std std-ref">DRM GPUVM locking section</span></a> and its internal
<code class="xref c c-func docutils literal notranslate"><span class="pre">get_next_vm_bo_from_list()</span></code> function.</p>
</section>
<section id="userptr-gpu-vmas">
<h2>userptr gpu_vmas<a class="headerlink" href="#userptr-gpu-vmas" title="Link to this heading">¶</a></h2>
<p>A userptr gpu_vma is a gpu_vma that, instead of mapping a buffer object to a
GPU virtual address range, directly maps a CPU mm range of anonymous-
or file page-cache pages.
A very simple approach would be to just pin the pages using
pin_user_pages() at bind time and unpin them at unbind time, but this
creates a Denial-Of-Service vector since a single user-space process
would be able to pin down all of system memory, which is not
desirable. (For special use-cases and assuming proper accounting pinning might
still be a desirable feature, though). What we need to do in the
general case is to obtain a reference to the desired pages, make sure
we are notified using a MMU notifier just before the CPU mm unmaps the
pages, dirty them if they are not mapped read-only to the GPU, and
then drop the reference.
When we are notified by the MMU notifier that CPU mm is about to drop the
pages, we need to stop GPU access to the pages by waiting for VM idle
in the MMU notifier and make sure that before the next time the GPU
tries to access whatever is now present in the CPU mm range, we unmap
the old pages from the GPU page tables and repeat the process of
obtaining new page references. (See the <a class="reference internal" href="#invalidation-example"><span class="std std-ref">notifier example</span></a> below). Note that when the core mm decides to
laundry pages, we get such an unmap MMU notification and can mark the
pages dirty again before the next GPU access. We also get similar MMU
notifications for NUMA accounting which the GPU driver doesn’t really
need to care about, but so far it has proven difficult to exclude
certain notifications.</p>
<p>Using a MMU notifier for device DMA (and other methods) is described in
<a class="reference internal" href="../core-api/pin_user_pages.html#mmu-notifier-registration-case"><span class="std std-ref">the pin_user_pages() documentation</span></a>.</p>
<p>Now, the method of obtaining struct page references using
get_user_pages() unfortunately can’t be used under a dma_resv lock
since that would violate the locking order of the dma_resv lock vs the
mmap_lock that is grabbed when resolving a CPU pagefault. This means
the gpu_vm’s list of userptr gpu_vmas needs to be protected by an
outer lock, which in our example below is the <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;lock</span></code>.</p>
<p>The MMU interval seqlock for a userptr gpu_vma is used in the following
way:</p>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="c1">// Exclusive locking mode here is strictly needed only if there are</span>
<span class="c1">// invalidated userptr gpu_vmas present, to avoid concurrent userptr</span>
<span class="c1">// revalidations of the same userptr gpu_vma.</span>
<span class="n">down_write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
<span class="nl">retry</span><span class="p">:</span>

<span class="c1">// Note: mmu_interval_read_begin() blocks until there is no</span>
<span class="c1">// invalidation notifier running anymore.</span>
<span class="n">seq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mmu_interval_read_begin</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="o">-&gt;</span><span class="n">userptr_interval</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">seq</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">gpu_vma</span><span class="o">-&gt;</span><span class="n">saved_seq</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">obtain_new_page_pointers</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">);</span>
<span class="w">        </span><span class="n">dma_resv_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="w">        </span><span class="n">add_gpu_vma_to_revalidate_list</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="p">);</span>
<span class="w">        </span><span class="n">dma_resv_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="w">        </span><span class="n">gpu_vma</span><span class="o">-&gt;</span><span class="n">saved_seq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// The usual revalidation goes here.</span>

<span class="c1">// Final userptr sequence validation may not happen before the</span>
<span class="c1">// submission dma_fence is added to the gpu_vm&#39;s resv, from the POW</span>
<span class="c1">// of the MMU invalidation notifier. Hence the</span>
<span class="c1">// userptr_notifier_lock that will make them appear atomic.</span>

<span class="n">add_dependencies</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_job</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>
<span class="n">down_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">userptr_notifier_lock</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">mmu_interval_read_retry</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vma</span><span class="o">-&gt;</span><span class="n">userptr_interval</span><span class="p">,</span><span class="w"> </span><span class="n">gpu_vma</span><span class="o">-&gt;</span><span class="n">saved_seq</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="n">up_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">userptr_notifier_lock</span><span class="p">);</span>
<span class="w">       </span><span class="k">goto</span><span class="w"> </span><span class="n">retry</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">job_dma_fence</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gpu_submit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_job</span><span class="p">));</span>

<span class="n">add_dma_fence</span><span class="p">(</span><span class="n">job_dma_fence</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>

<span class="n">for_each_external_obj</span><span class="p">(</span><span class="n">gpu_vm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">obj</span><span class="p">)</span>
<span class="w">       </span><span class="n">add_dma_fence</span><span class="p">(</span><span class="n">job_dma_fence</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">);</span>

<span class="n">dma_resv_unlock_all_resv_locks</span><span class="p">();</span>
<span class="n">up_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">userptr_notifier_lock</span><span class="p">);</span>
<span class="n">up_write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">lock</span><span class="p">);</span>
</pre></div>
</div>
<p>The code between <code class="docutils literal notranslate"><span class="pre">mmu_interval_read_begin()</span></code> and the
<code class="docutils literal notranslate"><span class="pre">mmu_interval_read_retry()</span></code> marks the read side critical section of
what we call the <code class="docutils literal notranslate"><span class="pre">userptr_seqlock</span></code>. In reality, the gpu_vm’s userptr
gpu_vma list is looped through, and the check is done for <em>all</em> of its
userptr gpu_vmas, although we only show a single one here.</p>
<p>The userptr gpu_vma MMU invalidation notifier might be called from
reclaim context and, again, to avoid locking order violations, we can’t
take any dma_resv lock nor the gpu_vm-&gt;lock from within it.</p>
<div class="highlight-C notranslate" id="invalidation-example"><div class="highlight"><pre><span></span><span class="kt">bool</span><span class="w"> </span><span class="nf">gpu_vma_userptr_invalidate</span><span class="p">(</span><span class="n">userptr_interval</span><span class="p">,</span><span class="w"> </span><span class="n">cur_seq</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">        </span><span class="c1">// Make sure the exec function either sees the new sequence</span>
<span class="w">        </span><span class="c1">// and backs off or we wait for the dma-fence:</span>

<span class="w">        </span><span class="n">down_write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">userptr_notifier_lock</span><span class="p">);</span>
<span class="w">        </span><span class="n">mmu_interval_set_seq</span><span class="p">(</span><span class="n">userptr_interval</span><span class="p">,</span><span class="w"> </span><span class="n">cur_seq</span><span class="p">);</span>
<span class="w">        </span><span class="n">up_write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">userptr_notifier_lock</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// At this point, the exec function can&#39;t succeed in</span>
<span class="w">        </span><span class="c1">// submitting a new job, because cur_seq is an invalid</span>
<span class="w">        </span><span class="c1">// sequence number and will always cause a retry. When all</span>
<span class="w">        </span><span class="c1">// invalidation callbacks, the mmu notifier core will flip</span>
<span class="w">        </span><span class="c1">// the sequence number to a valid one. However we need to</span>
<span class="w">        </span><span class="c1">// stop gpu access to the old pages here.</span>

<span class="w">        </span><span class="n">dma_resv_wait_timeout</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_vm</span><span class="o">-&gt;</span><span class="n">resv</span><span class="p">,</span><span class="w"> </span><span class="n">DMA_RESV_USAGE_BOOKKEEP</span><span class="p">,</span>
<span class="w">                              </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">MAX_SCHEDULE_TIMEOUT</span><span class="p">);</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When this invalidation notifier returns, the GPU can no longer be
accessing the old pages of the userptr gpu_vma and needs to redo the
page-binding before a new GPU submission can succeed.</p>
<section id="efficient-userptr-gpu-vma-exec-function-iteration">
<h3>Efficient userptr gpu_vma exec_function iteration<a class="headerlink" href="#efficient-userptr-gpu-vma-exec-function-iteration" title="Link to this heading">¶</a></h3>
<p>If the gpu_vm’s list of userptr gpu_vmas becomes large, it’s
inefficient to iterate through the complete lists of userptrs on each
exec function to check whether each userptr gpu_vma’s saved
sequence number is stale. A solution to this is to put all
<em>invalidated</em> userptr gpu_vmas on a separate gpu_vm list and
only check the gpu_vmas present on this list on each exec
function. This list will then lend itself very-well to the spinlock
locking scheme that is
<a class="reference internal" href="#spinlock-iteration"><span class="std std-ref">described in the spinlock iteration section</span></a>, since
in the mmu notifier, where we add the invalidated gpu_vmas to the
list, it’s not possible to take any outer locks like the
<code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;lock</span></code> or the <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;resv</span></code> lock. Note that the
<code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;lock</span></code> still needs to be taken while iterating to ensure the list is
complete, as also mentioned in that section.</p>
<p>If using an invalidated userptr list like this, the retry check in the
exec function trivially becomes a check for invalidated list empty.</p>
</section>
</section>
<section id="locking-at-bind-and-unbind-time">
<h2>Locking at bind and unbind time<a class="headerlink" href="#locking-at-bind-and-unbind-time" title="Link to this heading">¶</a></h2>
<p>At bind time, assuming a GEM object backed gpu_vma, each
gpu_vma needs to be associated with a gpu_vm_bo and that
gpu_vm_bo in turn needs to be added to the GEM object’s
gpu_vm_bo list, and possibly to the gpu_vm’s external object
list. This is referred to as <em>linking</em> the gpu_vma, and typically
requires that the <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;lock</span></code> and the <code class="docutils literal notranslate"><span class="pre">gem_object-&gt;gpuva_lock</span></code>
are held. When unlinking a gpu_vma the same locks should be held,
and that ensures that when iterating over <code class="docutils literal notranslate"><span class="pre">gpu_vmas`,</span> <span class="pre">either</span> <span class="pre">under</span>
<span class="pre">the</span> <span class="pre">``gpu_vm-&gt;resv</span></code> or the GEM object’s dma_resv, that the gpu_vmas
stay alive as long as the lock under which we iterate is not released. For
userptr gpu_vmas it’s similarly required that during vma destroy, the
outer <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;lock</span></code> is held, since otherwise when iterating over
the invalidated userptr list as described in the previous section,
there is nothing keeping those userptr gpu_vmas alive.</p>
</section>
<section id="locking-for-recoverable-page-fault-page-table-updates">
<h2>Locking for recoverable page-fault page-table updates<a class="headerlink" href="#locking-for-recoverable-page-fault-page-table-updates" title="Link to this heading">¶</a></h2>
<p>There are two important things we need to ensure with locking for
recoverable page-faults:</p>
<ul class="simple">
<li><p>At the time we return pages back to the system / allocator for
reuse, there should be no remaining GPU mappings and any GPU TLB
must have been flushed.</p></li>
<li><p>The unmapping and mapping of a gpu_vma must not race.</p></li>
</ul>
<p>Since the unmapping (or zapping) of GPU ptes is typically taking place
where it is hard or even impossible to take any outer level locks we
must either introduce a new lock that is held at both mapping and
unmapping time, or look at the locks we do hold at unmapping time and
make sure that they are held also at mapping time. For userptr
gpu_vmas, the <code class="docutils literal notranslate"><span class="pre">userptr_seqlock</span></code> is held in write mode in the mmu
invalidation notifier where zapping happens. Hence, if the
<code class="docutils literal notranslate"><span class="pre">userptr_seqlock</span></code> as well as the <code class="docutils literal notranslate"><span class="pre">gpu_vm-&gt;userptr_notifier_lock</span></code>
is held in read mode during mapping, it will not race with the
zapping. For GEM object backed gpu_vmas, zapping will take place under
the GEM object’s dma_resv and ensuring that the dma_resv is held also
when populating the page-tables for any gpu_vma pointing to the GEM
object, will similarly ensure we are race-free.</p>
<p>If any part of the mapping is performed asynchronously
under a dma-fence with these locks released, the zapping will need to
wait for that dma-fence to signal under the relevant lock before
starting to modify the page-table.</p>
<p>Since modifying the
page-table structure in a way that frees up page-table memory
might also require outer level locks, the zapping of GPU ptes
typically focuses only on zeroing page-table or page-directory entries
and flushing TLB, whereas freeing of page-table memory is deferred to
unbind or rebind time.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;The kernel development community.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.2.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/gpu/drm-vm-bind-locking.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>