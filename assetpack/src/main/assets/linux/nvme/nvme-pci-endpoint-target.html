<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2. NVMe PCI Endpoint Function Target &#8212; The Linux Kernel  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=a152c8ac" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Accounting" href="../accounting/index.html" />
    <link rel="prev" title="NVMe Subsystem" href="index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/logo.svg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">The Linux Kernel</a></h1>



<p class="blurb">6.16.0</p>







<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script><!-- SPDX-License-Identifier: GPL-2.0 -->

<p>
<h3 class="kernel-toc-contents">Contents</h3>
<input type="checkbox" class="kernel-toc-toggle" id = "kernel-toc-toggle" checked>
<label class="kernel-toc-title" for="kernel-toc-toggle"></label>

<div class="kerneltoc" id="kerneltoc">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../process/development-process.html">Development process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../process/submitting-patches.html">Submitting patches</a></li>
<li class="toctree-l1"><a class="reference internal" href="../process/code-of-conduct.html">Code of conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../maintainer/index.html">Maintainer handbook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../process/index.html">All development-process docs</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../core-api/index.html">Core API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../driver-api/index.html">Driver APIs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../subsystem-apis.html">Subsystems</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#core-subsystems">Core subsystems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#human-interfaces">Human interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#networking-interfaces">Networking interfaces</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../subsystem-apis.html#storage-interfaces">Storage interfaces</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../filesystems/index.html">Filesystems in the Linux kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../block/index.html">Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cdrom/index.html">CD-ROM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../scsi/index.html">SCSI Subsystem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../target/index.html">TCM Virtual Device</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">NVMe Subsystem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../subsystem-apis.html#other-subsystems">Other subsystems</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../locking/index.html">Locking</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../process/license-rules.html">Licensing rules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../doc-guide/index.html">Writing documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev-tools/index.html">Development tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev-tools/testing-overview.html">Testing guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel-hacking/index.html">Hacking guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trace/index.html">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fault-injection/index.html">Fault injection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../livepatch/index.html">Livepatching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rust/index.html">Rust</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../admin-guide/index.html">Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kbuild/index.html">Build system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../admin-guide/reporting-issues.html">Reporting issues</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/index.html">Userspace tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../userspace-api/index.html">Userspace API</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../firmware-guide/index.html">Firmware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../devicetree/index.html">Firmware and Devicetree</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../arch/index.html">CPU architectures</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../staging/index.html">Unsorted documentation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../translations/index.html">Translations</a></li>
</ul>

</div>

<script type="text/javascript"> <!--
  var sbar = document.getElementsByClassName("sphinxsidebar")[0];
  let currents = document.getElementsByClassName("current")
  if (currents.length) {
    sbar.scrollTop = currents[currents.length - 1].offsetTop;
  }
  --> </script>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/nvme/nvme-pci-endpoint-target.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <!-- SPDX-License-Identifier: GPL-2.0 -->
<!-- Copyright © 2023, Oracle and/or its affiliates. -->


<section id="nvme-pci-endpoint-function-target">
<h1><span class="section-number">2. </span>NVMe PCI Endpoint Function Target<a class="headerlink" href="#nvme-pci-endpoint-function-target" title="Link to this heading">¶</a></h1>
<dl class="field-list simple">
<dt class="field-odd">Author<span class="colon">:</span></dt>
<dd class="field-odd"><p>Damien Le Moal &lt;<a class="reference external" href="mailto:dlemoal&#37;&#52;&#48;kernel&#46;org">dlemoal<span>&#64;</span>kernel<span>&#46;</span>org</a>&gt;</p>
</dd>
</dl>
<p>The NVMe PCI endpoint function target driver implements a NVMe PCIe controller
using a NVMe fabrics target controller configured with the PCI transport type.</p>
<section id="overview">
<h2><span class="section-number">2.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">¶</a></h2>
<p>The NVMe PCI endpoint function target driver allows exposing a NVMe target
controller over a PCIe link, thus implementing an NVMe PCIe device similar to a
regular M.2 SSD. The target controller is created in the same manner as when
using NVMe over fabrics: the controller represents the interface to an NVMe
subsystem using a port. The port transfer type must be configured to be
“pci”. The subsystem can be configured to have namespaces backed by regular
files or block devices, or can use NVMe passthrough to expose to the PCI host an
existing physical NVMe device or a NVMe fabrics host controller (e.g. a NVMe TCP
host controller).</p>
<p>The NVMe PCI endpoint function target driver relies as much as possible on the
NVMe target core code to parse and execute NVMe commands submitted by the PCIe
host. However, using the PCI endpoint framework API and DMA API, the driver is
also responsible for managing all data transfers over the PCIe link. This
implies that the NVMe PCI endpoint function target driver implements several
NVMe data structure management and some NVMe command parsing.</p>
<ol class="arabic simple">
<li><p>The driver manages retrieval of NVMe commands in submission queues using DMA
if supported, or MMIO otherwise. Each command retrieved is then executed
using a work item to maximize performance with the parallel execution of
multiple commands on different CPUs. The driver uses a work item to
constantly poll the doorbell of all submission queues to detect command
submissions from the PCIe host.</p></li>
<li><p>The driver transfers completion queues entries of completed commands to the
PCIe host using MMIO copy of the entries in the host completion queue.
After posting completion entries in a completion queue, the driver uses the
PCI endpoint framework API to raise an interrupt to the host to signal the
commands completion.</p></li>
<li><p>For any command that has a data buffer, the NVMe PCI endpoint target driver
parses the command PRPs or SGLs lists to create a list of PCI address
segments representing the mapping of the command data buffer on the host.
The command data buffer is transferred over the PCIe link using this list of
PCI address segments using DMA, if supported. If DMA is not supported, MMIO
is used, which results in poor performance. For write commands, the command
data buffer is transferred from the host into a local memory buffer before
executing the command using the target core code. For read commands, a local
memory buffer is allocated to execute the command and the content of that
buffer is transferred to the host once the command completes.</p></li>
</ol>
<section id="controller-capabilities">
<h3><span class="section-number">2.1.1. </span>Controller Capabilities<a class="headerlink" href="#controller-capabilities" title="Link to this heading">¶</a></h3>
<p>The NVMe capabilities exposed to the PCIe host through the BAR 0 registers
are almost identical to the capabilities of the NVMe target controller
implemented by the target core code. There are some exceptions.</p>
<ol class="arabic simple">
<li><p>The NVMe PCI endpoint target driver always sets the controller capability
CQR bit to request “Contiguous Queues Required”. This is to facilitate the
mapping of a queue PCI address range to the local CPU address space.</p></li>
<li><p>The doorbell stride (DSTRB) is always set to be 4B</p></li>
<li><p>Since the PCI endpoint framework does not provide a way to handle PCI level
resets, the controller capability NSSR bit (NVM Subsystem Reset Supported)
is always cleared.</p></li>
<li><p>The boot partition support (BPS), Persistent Memory Region Supported (PMRS)
and Controller Memory Buffer Supported (CMBS) capabilities are never
reported.</p></li>
</ol>
</section>
<section id="supported-features">
<h3><span class="section-number">2.1.2. </span>Supported Features<a class="headerlink" href="#supported-features" title="Link to this heading">¶</a></h3>
<p>The NVMe PCI endpoint target driver implements support for both PRPs and SGLs.
The driver also implements IRQ vector coalescing and submission queue
arbitration burst.</p>
<p>The maximum number of queues and the maximum data transfer size (MDTS) are
configurable through configfs before starting the controller. To avoid issues
with excessive local memory usage for executing commands, MDTS defaults to 512
KB and is limited to a maximum of 2 MB (arbitrary limit).</p>
</section>
<section id="minimum-number-of-pci-address-mapping-windows-required">
<h3><span class="section-number">2.1.3. </span>Minimum number of PCI Address Mapping Windows Required<a class="headerlink" href="#minimum-number-of-pci-address-mapping-windows-required" title="Link to this heading">¶</a></h3>
<p>Most PCI endpoint controllers provide a limited number of mapping windows for
mapping a PCI address range to local CPU memory addresses. The NVMe PCI
endpoint target controllers uses mapping windows for the following.</p>
<ol class="arabic simple">
<li><p>One memory window for raising MSI or MSI-X interrupts</p></li>
<li><p>One memory window for MMIO transfers</p></li>
<li><p>One memory window for each completion queue</p></li>
</ol>
<p>Given the highly asynchronous nature of the NVMe PCI endpoint target driver
operation, the memory windows as described above will generally not be used
simultaneously, but that may happen. So a safe maximum number of completion
queues that can be supported is equal to the total number of memory mapping
windows of the PCI endpoint controller minus two. E.g. for an endpoint PCI
controller with 32 outbound memory windows available, up to 30 completion
queues can be safely operated without any risk of getting PCI address mapping
errors due to the lack of memory windows.</p>
</section>
<section id="maximum-number-of-queue-pairs">
<h3><span class="section-number">2.1.4. </span>Maximum Number of Queue Pairs<a class="headerlink" href="#maximum-number-of-queue-pairs" title="Link to this heading">¶</a></h3>
<p>Upon binding of the NVMe PCI endpoint target driver to the PCI endpoint
controller, BAR 0 is allocated with enough space to accommodate the admin queue
and multiple I/O queues. The maximum of number of I/O queues pairs that can be
supported is limited by several factors.</p>
<ol class="arabic simple">
<li><p>The NVMe target core code limits the maximum number of I/O queues to the
number of online CPUs.</p></li>
<li><p>The total number of queue pairs, including the admin queue, cannot exceed
the number of MSI-X or MSI vectors available.</p></li>
<li><p>The total number of completion queues must not exceed the total number of
PCI mapping windows minus 2 (see above).</p></li>
</ol>
<p>The NVMe endpoint function driver allows configuring the maximum number of
queue pairs through configfs.</p>
</section>
<section id="limitations-and-nvme-specification-non-compliance">
<h3><span class="section-number">2.1.5. </span>Limitations and NVMe Specification Non-Compliance<a class="headerlink" href="#limitations-and-nvme-specification-non-compliance" title="Link to this heading">¶</a></h3>
<p>Similar to the NVMe target core code, the NVMe PCI endpoint target driver does
not support multiple submission queues using the same completion queue. All
submission queues must specify a unique completion queue.</p>
</section>
</section>
<section id="user-guide">
<h2><span class="section-number">2.2. </span>User Guide<a class="headerlink" href="#user-guide" title="Link to this heading">¶</a></h2>
<p>This section describes the hardware requirements and how to setup an NVMe PCI
endpoint target device.</p>
<section id="kernel-requirements">
<h3><span class="section-number">2.2.1. </span>Kernel Requirements<a class="headerlink" href="#kernel-requirements" title="Link to this heading">¶</a></h3>
<p>The kernel must be compiled with the configuration options CONFIG_PCI_ENDPOINT,
CONFIG_PCI_ENDPOINT_CONFIGFS, and CONFIG_NVME_TARGET_PCI_EPF enabled.
CONFIG_PCI, CONFIG_BLK_DEV_NVME and CONFIG_NVME_TARGET must also be enabled
(obviously).</p>
<p>In addition to this, at least one PCI endpoint controller driver should be
available for the endpoint hardware used.</p>
<p>To facilitate testing, enabling the null-blk driver (CONFIG_BLK_DEV_NULL_BLK)
is also recommended. With this, a simple setup using a null_blk block device
as a subsystem namespace can be used.</p>
</section>
<section id="hardware-requirements">
<h3><span class="section-number">2.2.2. </span>Hardware Requirements<a class="headerlink" href="#hardware-requirements" title="Link to this heading">¶</a></h3>
<p>To use the NVMe PCI endpoint target driver, at least one endpoint controller
device is required.</p>
<p>To find the list of endpoint controller devices in the system:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># ls /sys/class/pci_epc/
 a40000000.pcie-ep
</pre></div>
</div>
<p>If PCI_ENDPOINT_CONFIGFS is enabled:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># ls /sys/kernel/config/pci_ep/controllers
 a40000000.pcie-ep
</pre></div>
</div>
<p>The endpoint board must of course also be connected to a host with a PCI cable
with RX-TX signal swapped. If the host PCI slot used does not have
plug-and-play capabilities, the host should be powered off when the NVMe PCI
endpoint device is configured.</p>
</section>
<section id="nvme-endpoint-device">
<h3><span class="section-number">2.2.3. </span>NVMe Endpoint Device<a class="headerlink" href="#nvme-endpoint-device" title="Link to this heading">¶</a></h3>
<p>Creating an NVMe endpoint device is a two step process. First, an NVMe target
subsystem and port must be defined. Second, the NVMe PCI endpoint device must
be setup and bound to the subsystem and port created.</p>
</section>
<section id="creating-a-nvme-subsystem-and-port">
<h3><span class="section-number">2.2.4. </span>Creating a NVMe Subsystem and Port<a class="headerlink" href="#creating-a-nvme-subsystem-and-port" title="Link to this heading">¶</a></h3>
<p>Details about how to configure a NVMe target subsystem and port are outside the
scope of this document. The following only provides a simple example of a port
and subsystem with a single namespace backed by a null_blk device.</p>
<p>First, make sure that configfs is enabled:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># mount -t configfs none /sys/kernel/config
</pre></div>
</div>
<p>Next, create a null_blk device (default settings give a 250 GB device without
memory backing). The block device created will be /dev/nullb0 by default:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># modprobe null_blk
# ls /dev/nullb0
/dev/nullb0
</pre></div>
</div>
<p>The NVMe PCI endpoint function target driver must be loaded:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># modprobe nvmet_pci_epf
# lsmod | grep nvmet
nvmet_pci_epf          32768  0
nvmet                 118784  1 nvmet_pci_epf
nvme_core             131072  2 nvmet_pci_epf,nvmet
</pre></div>
</div>
<p>Now, create a subsystem and a port that we will use to create a PCI target
controller when setting up the NVMe PCI endpoint target device. In this
example, the port is created with a maximum of 4 I/O queue pairs:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># cd /sys/kernel/config/nvmet/subsystems
# mkdir nvmepf.0.nqn
# echo -n &quot;Linux-pci-epf&quot; &gt; nvmepf.0.nqn/attr_model
# echo &quot;0x1b96&quot; &gt; nvmepf.0.nqn/attr_vendor_id
# echo &quot;0x1b96&quot; &gt; nvmepf.0.nqn/attr_subsys_vendor_id
# echo 1 &gt; nvmepf.0.nqn/attr_allow_any_host
# echo 4 &gt; nvmepf.0.nqn/attr_qid_max
</pre></div>
</div>
<p>Next, create and enable the subsystem namespace using the null_blk block
device:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># mkdir nvmepf.0.nqn/namespaces/1
# echo -n &quot;/dev/nullb0&quot; &gt; nvmepf.0.nqn/namespaces/1/device_path
# echo 1 &gt; &quot;nvmepf.0.nqn/namespaces/1/enable&quot;
</pre></div>
</div>
<p>Finally, create the target port and link it to the subsystem:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># cd /sys/kernel/config/nvmet/ports
# mkdir 1
# echo -n &quot;pci&quot; &gt; 1/addr_trtype
# ln -s /sys/kernel/config/nvmet/subsystems/nvmepf.0.nqn \
        /sys/kernel/config/nvmet/ports/1/subsystems/nvmepf.0.nqn
</pre></div>
</div>
</section>
<section id="creating-a-nvme-pci-endpoint-device">
<h3><span class="section-number">2.2.5. </span>Creating a NVMe PCI Endpoint Device<a class="headerlink" href="#creating-a-nvme-pci-endpoint-device" title="Link to this heading">¶</a></h3>
<p>With the NVMe target subsystem and port ready for use, the NVMe PCI endpoint
device can now be created and enabled. The NVMe PCI endpoint target driver
should already be loaded (that is done automatically when the port is created):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># ls /sys/kernel/config/pci_ep/functions
nvmet_pci_epf
</pre></div>
</div>
<p>Next, create function 0:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># cd /sys/kernel/config/pci_ep/functions/nvmet_pci_epf
# mkdir nvmepf.0
# ls nvmepf.0/
baseclass_code    msix_interrupts   secondary
cache_line_size   nvme              subclass_code
deviceid          primary           subsys_id
interrupt_pin     progif_code       subsys_vendor_id
msi_interrupts    revid             vendorid
</pre></div>
</div>
<p>Configure the function using any device ID (the vendor ID for the device will
be automatically set to the same value as the NVMe target subsystem vendor
ID):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># cd /sys/kernel/config/pci_ep/functions/nvmet_pci_epf
# echo 0xBEEF &gt; nvmepf.0/deviceid
# echo 32 &gt; nvmepf.0/msix_interrupts
</pre></div>
</div>
<p>If the PCI endpoint controller used does not support MSI-X, MSI can be
configured instead:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># echo 32 &gt; nvmepf.0/msi_interrupts
</pre></div>
</div>
<p>Next, let’s bind our endpoint device with the target subsystem and port that we
created:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># echo 1 &gt; nvmepf.0/nvme/portid
# echo &quot;nvmepf.0.nqn&quot; &gt; nvmepf.0/nvme/subsysnqn
</pre></div>
</div>
<p>The endpoint function can then be bound to the endpoint controller and the
controller started:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># cd /sys/kernel/config/pci_ep
# ln -s functions/nvmet_pci_epf/nvmepf.0 controllers/a40000000.pcie-ep/
# echo 1 &gt; controllers/a40000000.pcie-ep/start
</pre></div>
</div>
<p>On the endpoint machine, kernel messages will show information as the NVMe
target device and endpoint device are created and connected.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>null_blk: disk nullb0 created
null_blk: module loaded
nvmet: adding nsid 1 to subsystem nvmepf.0.nqn
nvmet_pci_epf nvmet_pci_epf.0: PCI endpoint controller supports MSI-X, 32 vectors
nvmet: Created nvm controller 1 for subsystem nvmepf.0.nqn for NQN nqn.2014-08.org.nvmexpress:uuid:2ab90791-2246-4fbb-961d-4c3d5a5a0176.
nvmet_pci_epf nvmet_pci_epf.0: New PCI ctrl &quot;nvmepf.0.nqn&quot;, 4 I/O queues, mdts 524288 B
</pre></div>
</div>
</section>
<section id="pci-root-complex-host">
<h3><span class="section-number">2.2.6. </span>PCI Root-Complex Host<a class="headerlink" href="#pci-root-complex-host" title="Link to this heading">¶</a></h3>
<p>Booting the PCI host will result in the initialization of the PCIe link (this
may be signaled by the PCI endpoint driver with a kernel message). A kernel
message on the endpoint will also signal when the host NVMe driver enables the
device controller:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>nvmet_pci_epf nvmet_pci_epf.0: Enabling controller
</pre></div>
</div>
<p>On the host side, the NVMe PCI endpoint function target device will is
discoverable as a PCI device, with the vendor ID and device ID as configured:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># lspci -n
0000:01:00.0 0108: 1b96:beef
</pre></div>
</div>
<p>An this device will be recognized as an NVMe device with a single namespace:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># lsblk
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
nvme0n1     259:0    0   250G  0 disk
</pre></div>
</div>
<p>The NVMe endpoint block device can then be used as any other regular NVMe
namespace block device. The <em>nvme</em> command line utility can be used to get more
detailed information about the endpoint device:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># nvme id-ctrl /dev/nvme0
NVME Identify Controller:
vid       : 0x1b96
ssvid     : 0x1b96
sn        : 94993c85650ef7bcd625
mn        : Linux-pci-epf
fr        : 6.13.0-r
rab       : 6
ieee      : 000000
cmic      : 0xb
mdts      : 7
cntlid    : 0x1
ver       : 0x20100
...
</pre></div>
</div>
</section>
</section>
<section id="endpoint-bindings">
<h2><span class="section-number">2.3. </span>Endpoint Bindings<a class="headerlink" href="#endpoint-bindings" title="Link to this heading">¶</a></h2>
<p>The NVMe PCI endpoint target driver uses the PCI endpoint configfs device
attributes as follows.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>vendorid</p></td>
<td><p>Ignored (the vendor id of the NVMe target subsystem is used)</p></td>
</tr>
<tr class="row-even"><td><p>deviceid</p></td>
<td><p>Anything is OK (e.g. PCI_ANY_ID)</p></td>
</tr>
<tr class="row-odd"><td><p>revid</p></td>
<td><p>Do not care</p></td>
</tr>
<tr class="row-even"><td><p>progif_code</p></td>
<td><p>Must be 0x02 (NVM Express)</p></td>
</tr>
<tr class="row-odd"><td><p>baseclass_code</p></td>
<td><p>Must be 0x01 (PCI_BASE_CLASS_STORAGE)</p></td>
</tr>
<tr class="row-even"><td><p>subclass_code</p></td>
<td><p>Must be 0x08 (Non-Volatile Memory controller)</p></td>
</tr>
<tr class="row-odd"><td><p>cache_line_size</p></td>
<td><p>Do not care</p></td>
</tr>
<tr class="row-even"><td><p>subsys_vendor_id</p></td>
<td><p>Ignored (the subsystem vendor id of the NVMe target subsystem
is used)</p></td>
</tr>
<tr class="row-odd"><td><p>subsys_id</p></td>
<td><p>Anything is OK (e.g. PCI_ANY_ID)</p></td>
</tr>
<tr class="row-even"><td><p>msi_interrupts</p></td>
<td><p>At least equal to the number of queue pairs desired</p></td>
</tr>
<tr class="row-odd"><td><p>msix_interrupts</p></td>
<td><p>At least equal to the number of queue pairs desired</p></td>
</tr>
<tr class="row-even"><td><p>interrupt_pin</p></td>
<td><p>Interrupt PIN to use if MSI and MSI-X are not supported</p></td>
</tr>
</tbody>
</table>
<p>The NVMe PCI endpoint target function also has some specific configurable
fields defined in the <em>nvme</em> subdirectory of the function directory. These
fields are as follows.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>mdts_kb</p></td>
<td><p>Maximum data transfer size in KiB (default: 512)</p></td>
</tr>
<tr class="row-even"><td><p>portid</p></td>
<td><p>The ID of the target port to use</p></td>
</tr>
<tr class="row-odd"><td><p>subsysnqn</p></td>
<td><p>The NQN of the target subsystem to use</p></td>
</tr>
</tbody>
</table>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;The kernel development community.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.2.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/nvme/nvme-pci-endpoint-target.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>